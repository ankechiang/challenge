{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part II-Feature Engineering\n",
    "\n",
    "### Feature Types\n",
    "- typ1: title to unigram word vector\n",
    "- typ2: title to bigram word vector\n",
    "- typ3: desc to unigram word vector\n",
    "- typ4: desc to bigram word vector\n",
    "\n",
    "### Output\n",
    "- unigram/bigram vocabularies: voc,count\n",
    "- scipy sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>short_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adana Gallery Suri Square Hijab – Light Pink</td>\n",
       "      <td>Material : Non sheer shimmer chiffonSizes : 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuba Heartbreaker Eau De Parfum Spray 100ml/3...</td>\n",
       "      <td>Formulated with oil-free hydrating botanicals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andoer 150cm Cellphone Smartphone Mini Dual-H...</td>\n",
       "      <td>150cm mini microphone compatible for iPhone v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANMYNA Complaint Silky Set 柔顺洗发配套 (Shampoo 52...</td>\n",
       "      <td>ANMYNA Complaint Silky Set (Shampoo 520ml + C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argital Argiltubo Green Clay For Face and Bod...</td>\n",
       "      <td>100% Authentic Rrefresh and brighten skin Ant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       Adana Gallery Suri Square Hijab – Light Pink   \n",
       "1   Cuba Heartbreaker Eau De Parfum Spray 100ml/3...   \n",
       "2   Andoer 150cm Cellphone Smartphone Mini Dual-H...   \n",
       "3   ANMYNA Complaint Silky Set 柔顺洗发配套 (Shampoo 52...   \n",
       "4   Argital Argiltubo Green Clay For Face and Bod...   \n",
       "\n",
       "                                          short_desc  \n",
       "0   Material : Non sheer shimmer chiffonSizes : 5...  \n",
       "1   Formulated with oil-free hydrating botanicals...  \n",
       "2   150cm mini microphone compatible for iPhone v...  \n",
       "3   ANMYNA Complaint Silky Set (Shampoo 520ml + C...  \n",
       "4   100% Authentic Rrefresh and brighten skin Ant...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('./training/data_train_no_html.csv', names=['country','uid','title','cat_lv1','cat_lv2','cat_lv3','short_desc','price','prod_type'], encoding='utf8')\n",
    "train_data = train_data.drop(['country','uid','cat_lv1','cat_lv2','cat_lv3','price','prod_type'], axis=1)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Type 1: word vectors - unigram (title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, u' ANMYNA Complaint Silky Set \\u67d4\\u987a\\u6d17\\u53d1\\u914d\\u5957 (Shampoo 520ml + Conditioner 250ml)')\n",
      "柔顺洗发配套 1\n",
      "治神經衰弱 1\n",
      "甜杏仁油 1\n",
      "石淋通片100 1\n",
      "逍遥丸 1\n",
      "钢丝梳 1\n",
      "雪绒花靓肤水嫩洁面乳 1\n",
      "해외배송 26\n"
     ]
    }
   ],
   "source": [
    "# title word vector - unigram\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "titles_cnt = count_vec.fit_transform(train_data[\"title\"])\n",
    "dist = np.sum(titles_cnt.toarray(), axis=0)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "titles_tfidf = tfidf_transformer.fit_transform(titles_cnt)\n",
    "titles_vocab = count_vec.get_feature_names()\n",
    "\n",
    "print(3,train_data[\"title\"][3])\n",
    "for v, count in zip(titles_vocab[33764:33775], dist[33764:33775]): print v, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33772 title unigrams\n"
     ]
    }
   ],
   "source": [
    "# vocalbulary list\n",
    "print len(titles_vocab), \"title unigrams\"\n",
    "import csv\n",
    "with open('./feature/unigram_title_vocab.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    for word, c in zip(titles_vocab,dist):\n",
    "        wr.writerow([word.encode('utf-8'),c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### type 2: word vectors - ngram (title), n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, u' ANMYNA Complaint Silky Set \\u67d4\\u987a\\u6d17\\u53d1\\u914d\\u5957 (Shampoo 520ml + Conditioner 250ml)')\n",
      "柔顺洗发配套 shampoo 1\n",
      "治神經衰弱 25 1\n",
      "石淋通片100 3boxes 1\n",
      "逍遥丸 治神經衰弱 1\n",
      "钢丝梳 发网 1\n",
      "雪绒花靓肤水嫩洁面乳 100g 1\n"
     ]
    }
   ],
   "source": [
    "# title word vector - bigram\n",
    "bigram_vec = CountVectorizer(ngram_range=(2, 2), min_df=1) \n",
    "titles_bigram_cnt = bigram_vec.fit_transform(train_data[\"title\"])\n",
    "dist = np.sum(titles_bigram_cnt.toarray(), axis=0)\n",
    "titles_bigram_vocab = bigram_vec.get_feature_names()\n",
    "titles_bigram_tfidf = tfidf_transformer.fit_transform(titles_bigram_cnt)\n",
    "\n",
    "print(3,train_data[\"title\"][3])\n",
    "for v, count in zip(titles_bigram_vocab[191059:191070], dist[191059:191070]): print v, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191065 title bigrams\n"
     ]
    }
   ],
   "source": [
    "# vocalbulary list\n",
    "print len(titles_bigram_vocab), \"title bigrams\"\n",
    "import csv\n",
    "with open('./feature/bigram_title_vocab.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    for word, count in zip(titles_bigram_vocab,dist):\n",
    "        wr.writerow([word.encode('utf-8'),count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### type 3: word vectors - unigram (desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "软化血管 1\n",
      "防止血栓形成 1\n",
      "阻斷黑色素形成 1\n",
      "降低血液浓度及血液粘稠度 1\n",
      "预防与治疗冠心病 1\n",
      "预防与治疗脑血栓 1\n",
      "高血糖 1\n"
     ]
    }
   ],
   "source": [
    "# desc word vector - unigram\n",
    "count_vec = CountVectorizer()\n",
    "desc_cnt = count_vec.fit_transform(train_data[\"short_desc\"])\n",
    "dist = np.sum(desc_cnt.toarray(), axis=0)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "desc_tfidf = tfidf_transformer.fit_transform(desc_cnt)\n",
    "desc_vocab = count_vec.get_feature_names()\n",
    "\n",
    "for v, count in zip(desc_vocab[45738:45745], dist[45738:45745]): print v, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45751 desc unigrams\n"
     ]
    }
   ],
   "source": [
    "# vocalbulary list\n",
    "print len(desc_vocab), \"desc unigrams\"\n",
    "import csv\n",
    "with open('./feature/unigram_desc_vocab.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    for word, count in zip(desc_vocab,dist):\n",
    "        wr.writerow([word.encode('utf-8'),count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### type 4: word vectors - bigram (desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# desc word vector - bigram\n",
    "bigram_vec = CountVectorizer(ngram_range=(2, 2), min_df=1)\n",
    "desc_bigram_cnt = bigram_vec.fit_transform(train_data[\"short_desc\"])\n",
    "#dist = np.sum(desc_bigram_cnt.toarray(), axis=0)\n",
    "desc_bigram_tfidf = tfidf_transformer.fit_transform(desc_bigram_cnt)\n",
    "bigram_desc_vocab = bigram_vec.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383607 desc bigrams\n"
     ]
    }
   ],
   "source": [
    "# vocalbulary list\n",
    "print len(bigram_desc_vocab), \"desc bigrams\"\n",
    "import csv\n",
    "with open('./feature/bigram_desc_vocab.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    for word in bigram_desc_vocab:\n",
    "        wr.writerow([word.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Output: load/save sparse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    x_coo = array.tocoo()\n",
    "    np.savez(filename, data = x_coo.data, row=x_coo.row, col=x_coo.col, shape=x_coo.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    from scipy import sparse\n",
    "    loader = np.load(filename)\n",
    "    return sparse.coo_matrix((loader['data'], (loader['row'], loader['col'])), shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr(\"unigram_title\", titles_tfidf)\n",
    "save_sparse_csr(\"bigram_title\", titles_bigram_tfidf)\n",
    "save_sparse_csr(\"unigram_desc\", desc_tfidf)\n",
    "save_sparse_csr(\"bigram_desc\", desc_bigram_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example: loading unigram_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 33772) (36283, 33772)\n",
      "  (0, 24307)\t0.204266817035\n",
      "  (0, 19735)\t0.213036224452\n",
      "  (0, 16806)\t0.374790199687\n",
      "  (0, 28670)\t0.284224364364\n",
      "  (0, 29366)\t0.493701265662\n",
      "  (0, 15291)\t0.45183799311\n",
      "  (0, 6064)\t0.493701265662 titles_tfidf[0]\n",
      "  (0, 6064)\t0.493701265662\n",
      "  (0, 15291)\t0.45183799311\n",
      "  (0, 16806)\t0.374790199687\n",
      "  (0, 19735)\t0.213036224452\n",
      "  (0, 24307)\t0.204266817035\n",
      "  (0, 28670)\t0.284224364364\n",
      "  (0, 29366)\t0.493701265662 test.tocsr()[0]\n"
     ]
    }
   ],
   "source": [
    "test = load_sparse_csr(\"unigram_title.npz\")\n",
    "print test.shape, titles_tfidf.shape\n",
    "print titles_tfidf[0], \"titles_tfidf[0]\"\n",
    "print test.tocsr()[0], \"test.tocsr()[0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
